{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Reverse Engineering the Mechanism of Wechat Red Envelop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generative Modeling\n",
    "For this section, we modify the diffusion model so as to learn the latent distribution behind the data and hence generate new data. The code supports running with GPU. The code is partially borrowed from the open tutorial resource published on github, and partial debugging is under the assistance of ChatGPT. We run the code on GPU of NVIDIA GeForce RTX 4070 Laptop, with the CPU of 13th Gen Intel(R) Core(TM) i9-13980HX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Code Implementaion\n",
    "The entire code is presented below. Again, it should be noted that the main idea of the neural network comes from diffusion model[1], partial code is borrowed from an open github repository[2], and partial debugging is under the assistance of ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load and preprocess data\n",
    "data = np.load('raw_data_1.npy')\n",
    "dataset = torch.Tensor(data).float().to(device)\n",
    "dimension = data.shape[1]\n",
    "print(\"The dimension of the data is: \", dimension, \" and the number of samples is: \", data.shape[0], 'The shape of the data is: ', data.shape)\n",
    "# Hyperparameters\n",
    "num_steps = 100  # Diffusion process time steps\n",
    "num_epochs = 4000  # Number of training epochs\n",
    "batch_size = 32 # Batch size\n",
    "\n",
    "# Create beta schedule (linearly increasing betas)\n",
    "betas = torch.linspace(1e-5, 0.5e-2, num_steps).to(device)\n",
    "\n",
    "# Compute alpha and related terms\n",
    "alphas = 1 - betas\n",
    "alphas_prod = torch.cumprod(alphas, dim=0)\n",
    "alphas_prod_p = torch.cat([torch.tensor([1.0], device=device), alphas_prod[:-1]], dim=0)\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod).to(device)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod).to(device)\n",
    "\n",
    "# Noise addition function in diffusion process\n",
    "def q_x(x_0, t):\n",
    "    noise = torch.randn_like(x_0).to(device)  # Standard normal noise\n",
    "    alphas_t = alphas_bar_sqrt[t].unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "    alphas_l_m_t = one_minus_alphas_bar_sqrt[t].unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "    return alphas_t * x_0 + alphas_l_m_t * noise\n",
    "\n",
    "# Checkpoint functions\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    \"\"\"Save model and optimizer state.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename, device):\n",
    "    \"\"\"Load model and optimizer state.\"\"\"\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Checkpoint loaded from {filename} at epoch {epoch} with loss {loss}\")\n",
    "    return epoch, loss\n",
    "\n",
    "# Define the MLP Diffusion model\n",
    "class MLPDiffusion(nn.Module):\n",
    "    def __init__(self, n_steps, dimension, num_units=128):\n",
    "        super(MLPDiffusion, self).__init__()\n",
    "        self.step_embedding = nn.Embedding(n_steps, num_units)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dimension + num_units, num_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_units, num_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_units, num_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_units, dimension),  # Output dimensions (x1, x2, x3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_embedding = self.step_embedding(t)  # Shape: (batch_size, num_units)\n",
    "        x = torch.cat([x, t_embedding], dim=1)  # Concatenate along feature dimension\n",
    "        return self.net(x)\n",
    "\n",
    "# Loss function: Mean Squared Error between predicted and true noise\n",
    "def diffusion_loss_fn(model, x_0, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, n_steps):\n",
    "    batch_size = x_0.shape[0]\n",
    "\n",
    "    # Randomly sample a timestep t for each example in the batch\n",
    "    t = torch.randint(0, n_steps, size=(batch_size,), device=device)\n",
    "\n",
    "    # Compute noisy input x(t)\n",
    "    a = alphas_bar_sqrt[t].unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "    e = torch.randn_like(x_0).to(device)  # Shape: (batch_size, 3)\n",
    "    aml = one_minus_alphas_bar_sqrt[t].unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "    x = a * x_0 + aml * e  # Shape: (batch_size, 3)\n",
    "\n",
    "    # Predict the noise using the model\n",
    "    output = model(x, t)  # Shape: (batch_size, 3)\n",
    "\n",
    "    # Compute the loss (MSE between true noise and predicted noise)\n",
    "    return ((e - output) ** 2).mean()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataset, optimizer, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, num_epochs):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_x in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = diffusion_loss_fn(model, batch_x, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, num_steps)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        epoch_loss /= len(dataloader.dataset)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "# Sampling functions\n",
    "def p_sample(model, x, t, betas, one_minus_alphas_bar_sqrt, device):\n",
    "    # Compute coefficients and reshape to (batch_size, 1) for broadcasting\n",
    "    coeff = (betas[t] / one_minus_alphas_bar_sqrt[t]).unsqueeze(1)  # Shape: (1000, 1)\n",
    "    inv_sqrt_one_minus_betas = (1 / torch.sqrt(1 - betas[t])).unsqueeze(1)  # Shape: (1000, 1)\n",
    "    sqrt_betas = torch.sqrt(betas[t]).unsqueeze(1)  # Shape: (1000, 1)\n",
    "    \n",
    "    # Predict the noise using the model\n",
    "    eps_theta = model(x, t)  # Shape: (1000, 3)\n",
    "    \n",
    "    # Compute the mean\n",
    "    mean = inv_sqrt_one_minus_betas * (x - coeff * eps_theta)  # Shape: (1000, 3)\n",
    "    \n",
    "    # Sample noise for the current step\n",
    "    z = torch.randn_like(x).to(device)  # Shape: (1000, 3)\n",
    "    \n",
    "    # Compute the sample\n",
    "    sample = mean + sqrt_betas * z  # Shape: (1000, 3)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def p_sample_loop(model, shape, n_steps, betas, one_minus_alphas_bar_sqrt, device):\n",
    "    \"\"\"\n",
    "    Generate samples by iteratively applying p_sample.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cur_x = torch.randn(shape).to(device)  # Initialize with random noise\n",
    "        x_seq = [cur_x]\n",
    "        for i in reversed(range(n_steps)):\n",
    "            t = torch.full((shape[0],), i, dtype=torch.long, device=device)  # Shape: (1000,)\n",
    "            cur_x = p_sample(model, cur_x, t, betas, one_minus_alphas_bar_sqrt, device)  # Shape: (1000, 3)\n",
    "            x_seq.append(cur_x)\n",
    "    return x_seq\n",
    "\n",
    "# Initialize model, optimizer, and train\n",
    "model = MLPDiffusion(num_steps, dimension, num_units=128).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "checkpoint_filename = 'checkpoint0.ckpt'\n",
    "\n",
    "# Train the model\n",
    "final_loss = train_model(model, dataset, optimizer, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, num_epochs)\n",
    "\n",
    "# Save the checkpoint\n",
    "save_checkpoint(model, optimizer, num_epochs, final_loss, checkpoint_filename)\n",
    "\n",
    "# Load the checkpoint (if needed)\n",
    "# load_checkpoint(model, optimizer, checkpoint_filename, device)\n",
    "\n",
    "# Generate samples\n",
    "generated_samples_seq = p_sample_loop(model, (1000, dimension), num_steps, betas, one_minus_alphas_bar_sqrt, device)\n",
    "\n",
    "# Extract the final generated samples\n",
    "generated_samples = generated_samples_seq[-1].cpu().detach().numpy()\n",
    "np.save('toy.npy', generated_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Methodology\n",
    "In the paper of diffusion model[1], the pseudocode of the paradigm on training and sampling is presented as belows:\n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <img src=\"image/pesudo1.png\"  style=\"margin-right: 10px;\">\n",
    "    <img src=\"image/pseudo2.png\" >\n",
    "</div>\n",
    "\n",
    "In the pseudocode above, $\\alpha_t = 1-\\beta_t$, $\\overline{\\alpha_t} = \\prod_{s-1}^t \\alpha_s$, where $\\beta$ denotes the beta schedule which increases linearly. $\\epsilon$ denotes noise and $x_0$ denotes the original data, meanwhile $x_T$ denotes that generated noise under normal distribution. Here we will apply the algorithm above to our specific task. \\\n",
    "The data stream and network component is presented in the figure below:\n",
    "\n",
    "![image](image/training_network.png)\n",
    "\n",
    "In the figure of training, $X_0 \\in R^{bs \\times d}$ is the input data distribution where $bs$ denotes batch size and $d$ denotes the dimension of the input data. Using the generated noise $\\epsilon$ and the original batch data under specific proportion, a noised sample is generated. The randomly chosen specific timestep t is embedded into latent space of dimension $ld$, and it will be concatenated with the noised sample. It should be noted that $ld$ is actually a hyperparameter. Then the concatenated tensor will be processed by three blocks which individually consists of a Linear Layer and a ReLU activation funcion. Finally, the processed tensor will be projected to the wanted dimension $d$ and is expected to be a close simulation of the original generated noise $\\epsilon$. Here we use the MSE Loss as the loss function for backpropogation. We emphasize that during training, the time step t is randomly chosen, not orderly chosen, so as to fully maximize every batch of data since in this way every batch data can be cooperated with a different timestep t which, to some extent, improve the robustness of the network. The noise predictor in this implementation consists of four linear layers, three activation functions, and involves an embedding layer for projecting discrete timestep into continuous latent space.\n",
    "\n",
    "![image](image/sampling.png)\n",
    "\n",
    "In the figure of sampling, we firstly generate a noise tensor with dimension $num \\times d$, which subdues normal dirtribution. Then starting from the timestep t to 1, we will input the noised distribution and the timestep into the noise predictor to generated a simulated noise. Then we will use the noised data and simulated noise to generated the data one step ahead, in the proportion illustrated in the pseudocode of sampling. The generated $X_{t-1}$ will be the input of the next block, together with timestep $t-1$ correspondingly. This process will be repeated $t$ times until $X_0$ is generated, which should be expected to yield the same distribution of the input data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Experiment\n",
    "Unless additionally stated, we set up 4000 epochs and 100 timesteps. The dimension of the hidden space for timestep $t$ is 128, and the batch size is set to 32 with dataloader's shuffling mode on. It is noticeable that the timestep isn't rather large, since it is generally considered that the data distribution is not way too complicated and excessive training and inferencing isn't necessary. \n",
    "### Result and Visualization on 3D Experiment\n",
    "We train our model on the dataset of $R^{100, 3}$, i.e., on the dataset consisting of 100 data on 3 people's money while the total amount of money is 5, specifically speaking. Since the data visualization for three dimensional dataset is quite accessable, here we visualize the data distribution of the original dataset and generated dataset that comprises 1000 pieces of data, respectively. \n",
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <img src=\"image/vis_collected.png\"  style=\"margin-right: 10px;\">\n",
    "    <img src=\"image/dif_col_generated_1.png\" style=\"margin-right: 10px;\" >\n",
    "    <img src=\"image/dif_col_generated_2.png\"  style=\"margin-right: 10px;\">\n",
    "</div>\n",
    "\n",
    "From the figure above, starting from left to right, the pictures are about the visualization for the original data, the visualization the generated data, and another view into the visualization of the generated data. It is manifest that the simulated data subdues the distribution of the orignal data well, which is indicated by the tiny jittering of the plane in the figure. This is intuitive that the simulation is quite good since the hidden distribution of the original data is not obscure, and the network can easily learn the linear paterns behind the data. Not to further mention that the data is only three dimensional. \n",
    "### Result and Visualization on 4D experiment\n",
    "We also train our model on a (100, 4) dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
